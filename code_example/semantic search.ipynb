{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2b5270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the cosine-similarity between the query and all entries in the corpus.\n",
    "# embedding techniques are used to represent words/text mathematically with numeric vectors using encoders/transformer\n",
    "# such as  one-hot encoding..\n",
    "# SentenceTransformer('all-MiniLM-L6-v2') defines which embedding transformer model we like to use.\n",
    "# In this example, we load all-MiniLM-L6-v2, which is a MiniLM model fine tuned on a large dataset of over 1 billion training\n",
    "# pairs.\n",
    "# !pip install -U sentence-transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Corpus with example sentences\n",
    "corpus = ['A man is eating food.',\n",
    "          'A man is eating a piece of bread.',\n",
    "          'The girl is carrying a baby.',\n",
    "          'A man is riding a horse.',\n",
    "          'A woman is playing violin.',\n",
    "          'Two men pushed carts through the woods.',\n",
    "          'A man is riding a white horse on an enclosed ground.',\n",
    "          'A monkey is playing drums.',\n",
    "          'A cheetah is running behind its prey.'\n",
    "          ]\n",
    "embeddings =encoder.encode(corpus, convert_to_tensor=True)\n",
    "\n",
    "# Query sentences:\n",
    "queries = ['A man is eating pasta.', 'Someone in a gorilla costume is playing a set of drums.',\n",
    "           'A cheetah chases prey on across a field.']\n",
    "\n",
    "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "# top_k = min(5, len(corpus)) # for cases where the corpus is shorter than 5\n",
    "for q in queries:\n",
    "    query_embedding = encoder.encode(q, convert_to_tensor=True)\n",
    "\n",
    "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
    "    cos_scores = util.cos_sim(query_embedding, embeddings)[0]\n",
    "    top_results = torch.topk(cos_scores, k=5)\n",
    "\n",
    "    print(\"Query:\", q)\n",
    "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    for score, idx in zip(top_results[0], top_results[1]):\n",
    "        print(corpus[idx], \"(Score: {:.4f})\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f4588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use util.semantic_search instead\n",
    "# util.semantic_search performs a cosine similarity search between a list of query embeddings and a list of corpus embeddings.\n",
    "# It can be used for Information Retrieval / Semantic Search for corpora up to about 1 Million entries.\n",
    "# By default, up to 100 queries are processed in parallel.\n",
    "# Further, the corpus is chunked into set of up to 500k entries. \n",
    "# You can increase query_chunk_size and corpus_chunk_size, which leads to increased speed for large corpora,\n",
    "# but also increases the memory requirement.\n",
    "# returns a list with one entry for each query.\n",
    "# Each entry is a dictionaries with the keys ‘corpus_id’ and ‘score’, \n",
    "# sorted by decreasing cosine similarity scores.\n",
    "query_embeddings =[]\n",
    "for q in queries:\n",
    "    query_embedding = encoder.encode(q, convert_to_tensor=True)\n",
    "    query_embeddings.append(query_embedding)\n",
    "    \n",
    "sim_scores = util.semantic_search(query_embeddings, embeddings, top_k = 5)\n",
    "print(sim_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47785837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text summarization on congress bills (draft laws) dataset\n",
    "import pandas as pd\n",
    "WH_legis= pd.read_csv('house_legislation_116.csv')\n",
    "# 'summary' column contains a summary of the bill\n",
    "print(WH_legis['summary'][0])\n",
    "WH_legis = WH_legis[WH_legis['summary'].notna()] # drop Nan rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bda031",
   "metadata": {},
   "outputs": [],
   "source": [
    "WH_legis.reset_index(inplace =True)\n",
    "summary= WH_legis['summary']\n",
    "queries =['medication pricing', 'foreigner residence in united states', 'climate change']\n",
    "query_embeddings =[]\n",
    "for q in queries:\n",
    "    query_embedding = encoder.encode(q, convert_to_tensor=True)\n",
    "    query_embeddings.append(query_embedding)\n",
    "corpus_embeddings =encoder.encode(summary, convert_to_tensor=True) \n",
    "sim_cos_scores = util.semantic_search(query_embeddings, corpus_embeddings, top_k = 10)# it takes some time\n",
    "\n",
    "print(sim_cos_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfcb6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate Nearest Neighbor (ANN) can be helpful, since the data is partitioned into smaller fractions of similar embeddings.\n",
    "# that is KNN is first applied to group the data based on similarity.\n",
    "# the index forest can be searched efficiently and the embeddings with the highest similarity (the nearest neighbors) can be retrieved within milliseconds,\n",
    "# even if you have millions of vectors.The main disadvatage is that some vectors with high similarity may be missed; that's\n",
    "# why this is called Approximate Nearest Neighbor\n",
    "#For all ANN methods, there are usually one or more parameters to tune that determine the recall-speed trade-off.\n",
    "#If you want the highest speed, you have a high chance of missing hits. If you want high recall, the search speed decreases.\n",
    "# AnnoyIndex() takes an argument represting the embedding size ;the number of features in an indexed vector;get the min embedding length\n",
    "\n",
    "# install annoy\n",
    "!pip install annoy\n",
    "from annoy import AnnoyIndex\n",
    "# [min(len(emb) for emb in corpus_embeddings)]# result in 384\n",
    "embedding_size = 384\n",
    "n_tree= 200 # No. of clusters\n",
    "annoy_index = AnnoyIndex(embedding_size, 'angular')\n",
    "for i in range(len(corpus_embeddings)):\n",
    "        annoy_index.add_item(i, corpus_embeddings[i])\n",
    "\n",
    "annoy_index.build(n_tree) #apply ANN to build a forest of index trees (200 trees)\n",
    "#annoy_index.save(annoy_index_path)# to save the ANN model to a file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332aa604",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_hits =5 \n",
    "for q in queries:\n",
    "    query_embedding = encoder.encode(q, convert_to_tensor=True)\n",
    "    #Search the 5 closest items.\n",
    "    #include_distances: The flag indicating whether to returns all corresponding distances.\n",
    "    corpus_ids, scores = annoy_index.get_nns_by_vector(query_embedding, top_k_hits, include_distances=True)\n",
    "    hits = []\n",
    "    for i, score in zip(corpus_ids, scores):\n",
    "        ##  the scores returned by Annoy_index is euclidean distance,\n",
    "        # we need to calculate the cosine distance(in case comparison with other cosine similarity methods) \n",
    "        # the cosine distance is equals to 1 - e^2/2, where e is the euclidean distance value\n",
    "        hits.append({'corpus_id': i, 'score': 1-((score**2) / 2)})\n",
    "    print(\"\\n Input question:\", q)\n",
    "    for hit in hits[0:top_k_hits]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], summary[hit['corpus_id']]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
