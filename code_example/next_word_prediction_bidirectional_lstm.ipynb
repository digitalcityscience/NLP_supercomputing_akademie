{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5fcf0cd",
   "metadata": {},
   "source": [
    "# Sequence Prediction Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317ca610",
   "metadata": {},
   "source": [
    "To train the bidirectional LSTM model, we use Twitter dataset <a href=\"https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip\" target=\"_blank\">here</a>. We need to split the text line by line so that we can generate sequences from the text using the tokenizer. Letâ€™s first import the data and take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8b434ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/ariel/Documents/Jupyter/code_example/en_US_twitter.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/en_US_twitter.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# read and convert from one block text into a list of lines \u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcurrent_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     21\u001b[0m      lines \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#data = open(path, encoding='utf-8').read().lower()\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#text = pd.read_csv('medium_data.csv')\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#data# = data[:1000000]\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#text['title'] = text['title'].apply(lambda x: x.replace(u'\\xa0',u' '))\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#text['title'] = text['title'].apply(lambda x: x.replace('\\u200a',' '))\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/ariel/Documents/Jupyter/code_example/en_US_twitter.txt'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import heapq\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "import os \n",
    "current_dir = os.getcwd()\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "#path ='text.txt'\n",
    "path = r\"/en_US_twitter.txt\"\n",
    "# read and convert from one block text into a list of lines \n",
    "with open(current_dir + path, encoding='utf-8') as f:\n",
    "     lines = [line.rstrip('\\n') for line in f]\n",
    "#data = open(path, encoding='utf-8').read().lower()\n",
    "#text = pd.read_csv('medium_data.csv')\n",
    "#data# = data[:1000000]\n",
    "\n",
    "#text['title'] = text['title'].apply(lambda x: x.replace(u'\\xa0',u' '))\n",
    "#text['title'] = text['title'].apply(lambda x: x.replace('\\u200a',' '))\n",
    "\n",
    "text =lines[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be7c5483",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtext\u001b[49m[:\u001b[38;5;241m5\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "text[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b4636b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ce17b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total input sequences:  58427\n"
     ]
    }
   ],
   "source": [
    "#oov_token: if given, it will be added to word_index and used to replace out-of-vocabulary words during text_to_sequence calls For those words which are not found in word_index (vocabulary)\n",
    "tokenizer = Tokenizer(oov_token='<oov>') \n",
    "tokenizer.fit_on_texts(text)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "input_sequences = []\n",
    "for line in text:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]  \n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "print(\"Total input sequences: \", len(input_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26cc17e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 54, 22,  6])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sequence padding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "input_sequences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "080f921d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a2418e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1826/1826 [==============================] - 192s 104ms/step - loss: 7.3348 - accuracy: 0.0382\n",
      "Epoch 2/50\n",
      "1826/1826 [==============================] - 188s 103ms/step - loss: 6.6876 - accuracy: 0.0637\n",
      "Epoch 3/50\n",
      "1826/1826 [==============================] - 194s 106ms/step - loss: 6.2560 - accuracy: 0.0847\n",
      "Epoch 4/50\n",
      "1826/1826 [==============================] - 201s 110ms/step - loss: 5.8215 - accuracy: 0.1013\n",
      "Epoch 5/50\n",
      "1826/1826 [==============================] - 196s 107ms/step - loss: 5.3644 - accuracy: 0.1205\n",
      "Epoch 6/50\n",
      "1826/1826 [==============================] - 196s 107ms/step - loss: 4.9059 - accuracy: 0.1501\n",
      "Epoch 7/50\n",
      "1826/1826 [==============================] - 196s 107ms/step - loss: 4.4605 - accuracy: 0.1949\n",
      "Epoch 8/50\n",
      "1826/1826 [==============================] - 191s 104ms/step - loss: 4.0379 - accuracy: 0.2496\n",
      "Epoch 9/50\n",
      "1826/1826 [==============================] - 195s 107ms/step - loss: 3.6459 - accuracy: 0.3065\n",
      "Epoch 10/50\n",
      "1826/1826 [==============================] - 195s 107ms/step - loss: 3.2921 - accuracy: 0.3619\n",
      "Epoch 11/50\n",
      "1826/1826 [==============================] - 195s 107ms/step - loss: 2.9712 - accuracy: 0.4153\n",
      "Epoch 12/50\n",
      "1826/1826 [==============================] - 189s 104ms/step - loss: 2.6866 - accuracy: 0.4635\n",
      "Epoch 13/50\n",
      "1826/1826 [==============================] - 191s 104ms/step - loss: 2.4310 - accuracy: 0.5111\n",
      "Epoch 14/50\n",
      "1826/1826 [==============================] - 192s 105ms/step - loss: 2.2014 - accuracy: 0.5534\n",
      "Epoch 15/50\n",
      "1826/1826 [==============================] - 193s 106ms/step - loss: 1.9942 - accuracy: 0.5915\n",
      "Epoch 16/50\n",
      "1826/1826 [==============================] - 194s 106ms/step - loss: 1.8091 - accuracy: 0.6284\n",
      "Epoch 17/50\n",
      "1826/1826 [==============================] - 184s 101ms/step - loss: 1.6453 - accuracy: 0.6604\n",
      "Epoch 18/50\n",
      "1826/1826 [==============================] - 187s 103ms/step - loss: 1.4963 - accuracy: 0.6901\n",
      "Epoch 19/50\n",
      "1826/1826 [==============================] - 189s 104ms/step - loss: 1.3686 - accuracy: 0.7162\n",
      "Epoch 20/50\n",
      "1826/1826 [==============================] - 183s 100ms/step - loss: 1.2519 - accuracy: 0.7398\n",
      "Epoch 21/50\n",
      "1826/1826 [==============================] - 187s 103ms/step - loss: 1.1449 - accuracy: 0.7643\n",
      "Epoch 22/50\n",
      "1826/1826 [==============================] - 191s 105ms/step - loss: 1.0536 - accuracy: 0.7802\n",
      "Epoch 23/50\n",
      "1826/1826 [==============================] - 191s 104ms/step - loss: 0.9707 - accuracy: 0.7987\n",
      "Epoch 24/50\n",
      "1826/1826 [==============================] - 193s 106ms/step - loss: 0.8942 - accuracy: 0.8152\n",
      "Epoch 25/50\n",
      "1826/1826 [==============================] - 182s 100ms/step - loss: 0.8305 - accuracy: 0.8280\n",
      "Epoch 26/50\n",
      "1826/1826 [==============================] - 186s 102ms/step - loss: 0.7719 - accuracy: 0.8402\n",
      "Epoch 27/50\n",
      "1826/1826 [==============================] - 189s 103ms/step - loss: 0.7218 - accuracy: 0.8493\n",
      "Epoch 28/50\n",
      "1826/1826 [==============================] - 182s 100ms/step - loss: 0.6743 - accuracy: 0.8595\n",
      "Epoch 29/50\n",
      "1826/1826 [==============================] - 184s 101ms/step - loss: 0.6351 - accuracy: 0.8675\n",
      "Epoch 30/50\n",
      "1826/1826 [==============================] - 184s 101ms/step - loss: 0.6017 - accuracy: 0.8729\n",
      "Epoch 31/50\n",
      "1826/1826 [==============================] - 187s 103ms/step - loss: 0.5667 - accuracy: 0.8800\n",
      "Epoch 32/50\n",
      "1826/1826 [==============================] - 193s 106ms/step - loss: 0.5396 - accuracy: 0.8844\n",
      "Epoch 33/50\n",
      "1826/1826 [==============================] - 193s 106ms/step - loss: 0.5153 - accuracy: 0.8894\n",
      "Epoch 34/50\n",
      "1826/1826 [==============================] - 191s 105ms/step - loss: 0.4966 - accuracy: 0.8916\n",
      "Epoch 35/50\n",
      "1826/1826 [==============================] - 192s 105ms/step - loss: 0.4777 - accuracy: 0.8943\n",
      "Epoch 36/50\n",
      "1826/1826 [==============================] - 185s 101ms/step - loss: 0.4586 - accuracy: 0.8982\n",
      "Epoch 37/50\n",
      "1826/1826 [==============================] - 186s 102ms/step - loss: 0.4454 - accuracy: 0.9010\n",
      "Epoch 38/50\n",
      "1826/1826 [==============================] - 189s 104ms/step - loss: 0.4365 - accuracy: 0.9006\n",
      "Epoch 39/50\n",
      "1826/1826 [==============================] - 188s 103ms/step - loss: 0.4242 - accuracy: 0.9036\n",
      "Epoch 40/50\n",
      "1826/1826 [==============================] - 188s 103ms/step - loss: 0.4123 - accuracy: 0.9050\n",
      "Epoch 41/50\n",
      "1826/1826 [==============================] - 191s 105ms/step - loss: 0.4068 - accuracy: 0.9049\n",
      "Epoch 42/50\n",
      "1826/1826 [==============================] - 189s 104ms/step - loss: 0.3989 - accuracy: 0.9067\n",
      "Epoch 43/50\n",
      "1826/1826 [==============================] - 194s 107ms/step - loss: 0.3861 - accuracy: 0.9085\n",
      "Epoch 44/50\n",
      "1826/1826 [==============================] - 193s 106ms/step - loss: 0.3835 - accuracy: 0.9083\n",
      "Epoch 45/50\n",
      "1826/1826 [==============================] - 189s 104ms/step - loss: 0.3818 - accuracy: 0.9087\n",
      "Epoch 46/50\n",
      "1826/1826 [==============================] - 189s 104ms/step - loss: 0.3734 - accuracy: 0.9100\n",
      "Epoch 47/50\n",
      "1826/1826 [==============================] - 188s 103ms/step - loss: 0.3659 - accuracy: 0.9106\n",
      "Epoch 48/50\n",
      "1826/1826 [==============================] - 191s 104ms/step - loss: 0.3625 - accuracy: 0.9109\n",
      "Epoch 49/50\n",
      "1826/1826 [==============================] - 194s 106ms/step - loss: 0.3641 - accuracy: 0.9095\n",
      "Epoch 50/50\n",
      "1826/1826 [==============================] - 192s 105ms/step - loss: 0.3610 - accuracy: 0.9104\n",
      "<keras.engine.sequential.Sequential object at 0x0000015111BEADF0>\n"
     ]
    }
   ],
   "source": [
    "# we will use here bidirectional LSTM, which learn sequences in both directions.\n",
    "# An embedding layer is added to map input words into dense vectors. this layer is very important since\n",
    "# it allows the network to learn more about the relationship between inputs and to process the data more efficiently.\n",
    "# each word in our model will have an embedding vector of size 100. \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(150)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "adam = Adam(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "history = model.fit(xs, ys, epochs=50, verbose=1)\n",
    "#print model.summary()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc66740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting model accuracy and loss\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.show()\n",
    "plot_graphs(history, 'accuracy')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912de0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c464ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "!mkdir -p saved_model    \n",
    "model.save('saved_model/my_model_twitter')  \n",
    "#del model  # deletes the existing model\n",
    "    \n",
    "# relaod the saved model \n",
    "saved_model2 = load_model('saved_model/my_model_twitter')\n",
    "#new_model.evaluate(test_images, test_labels, verbose=2)# to evaluate the model with new data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3190361a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it seem like bat can\n"
     ]
    }
   ],
   "source": [
    "#Predicting next word of title\n",
    "tokenizer = Tokenizer(oov_token='<oov>') \n",
    "tokenizer.fit_on_texts(text)\n",
    "seed_text = \"it seem like\"\n",
    "seed_text_2= \"we've decided\"\n",
    "num_words = 2\n",
    "  \n",
    "for _ in range(num_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = saved_model2.predict(token_list, verbose=0)\n",
    "    class_x =np.argmax(predicted,axis=1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == class_x:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "851007c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[None, None]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9448482",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m     plt\u001b[38;5;241m.\u001b[39mylabel(string)\n\u001b[0;32m      7\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m----> 8\u001b[0m \u001b[43mplot_graphs\u001b[49m\u001b[43m(\u001b[49m\u001b[43msaved_model2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 4\u001b[0m, in \u001b[0;36mplot_graphs\u001b[1;34m(history, string)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_graphs\u001b[39m(history, string):\n\u001b[1;32m----> 4\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      5\u001b[0m     plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     plt\u001b[38;5;241m.\u001b[39mylabel(string)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'accuracy'"
     ]
    }
   ],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1d8b28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
