{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a69b9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use climate time series data (daily):https://www.kaggle.com/datasets/sumanthvrao/daily-climate-time-series-data\n",
    "# install scalecast library\n",
    "!pip install scalecast --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb69a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scalecast\n",
    "import tensorflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scalecast.Forecaster import Forecaster\n",
    "# If a column or index cannot be represented as an array of datetimes,\n",
    "#say because of an unparsable value or a mixture of timezones,\n",
    "# the column or index will be returned unaltered as an object data type.\n",
    "\n",
    "df = pd.read_csv('DailyDelhiClimateTrain.csv',parse_dates=['date'])\n",
    "# take a look at the data\n",
    "df.head()\n",
    "#len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9592f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we must first call the Forecaster object with the y and current_dates parameters as 'meantemp' and 'date' variable specified\n",
    "f = Forecaster( y=df['meantemp'], current_dates=df['date'] )\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8c814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let’s decompose this time series by viewing the PACF (Partial Auto Correlation Function) plot,\n",
    "# which measures how much the y variable(meantemp) is correlated to past values of itself.\n",
    "# blue area PACF plots depicts the significance threshold. \n",
    "# That means, lags that located within this area is statistically close to zero and thus insignificant autocorrelation\n",
    "# between data points. \n",
    "f.plot_pacf(lags=30)# up to 30 lags\n",
    "plt.show()\n",
    "# PACF will depicts intuitively correlations of 1  at  lag  0,\n",
    "# since this represents the correlation of the time series with itself.\n",
    "#this plot indicate significant autocorrelation at lag 1 which means that adjacent points (have lag of 1) are highly correlated\n",
    "# there are non zero autocorrelation at different lags as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1c43fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s further decompose the series into its trend, seasonal, and residual parts:\n",
    "from matplotlib import pyplot\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "result = seasonal_decompose(df['meantemp'],  model='multiplicative', period=365)# requency of the observations is 1\n",
    "result.plot()\n",
    "pyplot.show()\n",
    "#The figure obviously indicates yearly seasonality and increasing trend over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cbfb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let’s test the series’ stationarity.\n",
    "# If bool (full_res = False), returns whether the test suggests stationarity.\n",
    "# null Hypothesis of Augmented Dickey-Fuller (ADF) test: time series is non_stationary ( there is a unit root,)\n",
    "# If the pvalue is above a critical size (Default is 0.05), then we cannot reject that there is a unit root.\n",
    "stat = f.adf_test(full_res=True)\n",
    "print(stat)\n",
    "\n",
    "# p_value is 0.28 > 0.0.5, then we cannot reject null hypothesis and thus time series is non stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa3a48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, to call an LSTM forecast. By default, \n",
    "# this model will be run with a single input layer of 8 units, Adam optimizer, tanh activation,\n",
    "#  a learning rate of 0.001, and no dropout.\n",
    "\n",
    "# generate future dates: The number of dates you generate in this step will determine how long all models will be forecast out.\n",
    "f.set_validation_metric('mape')\n",
    "f.set_test_length(30)       #   30 observations to test the results\n",
    "f.generate_future_dates(30) #  30 future points to forecast\n",
    "f.set_estimator('lstm')     #  LSTM neural network\n",
    "f.manual_forecast(call_me='lstm_10lags_epochs5',lags=10, epochs=5)\n",
    "f.plot_test_set(ci=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045bc465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All data is scaled going into the model with a min-max scaler and un-scaled coming out.\n",
    "#Anything you can pass to the fit() method in TensorFlow,\n",
    "# you can also pass to the scalecast manual_forecast() method.\n",
    "#Plots all test-set predictions with the actuals.\n",
    "#ci (bool) – Default False. Whether to display the confidence intervals.\n",
    "# 5 lags, since we noticed 5 days autocorrelation \n",
    "\n",
    "# let’s try increasing the number of layers in the network to 4,\n",
    "#increasing epochs to 10, but monitoring the validation loss value and telling the model to quit after more\n",
    "#than 5 iterations in which that doesn’t improve. This is known as early stopping.\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "f.manual_forecast(\n",
    "    call_me='lstm_5lags_20epochs_4layers',\n",
    "    lags=5,\n",
    "    epochs=20,\n",
    "    batch_size=16,\n",
    "    activation='tanh',\n",
    "    optimizer='Adam',\n",
    "    shuffle=True,\n",
    "    learning_rate=0.01,\n",
    "\n",
    "    lstm_layer_sizes=(72,)*4, # 4 layers, each 72 units (size)\n",
    "     dropout=(0,)*4, # dropout rate for each layer\n",
    "     plot_loss=True\n",
    ")\n",
    "\n",
    "f.plot_test_set(ci=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594568f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.plot_test_set(ci=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060e5e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the best 2 models based on MAPE metric\n",
    "#f.plot_test_set(order_by='LevelTestSetMAPE',models='top_2',ci=True)## MAPE metric is used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ec740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets have a look on the statistics of our models\n",
    "res = f.export(dfs=['model_summaries'])\n",
    "models =res['ModelNickname']\n",
    "for m in models:\n",
    "    print(m,  res.loc[res['ModelNickname'] == m, 'LevelTestSetMAPE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1ef624",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
